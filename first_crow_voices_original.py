# -*- coding: utf-8 -*-
"""first-crow-voices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGIsMvUG6sSAfcZg7gI5EHGmlnmaaYkH
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import librosa
import soundfile as sf

# Parameters (adjust these based on your model and data)
latent_dim = 32  # Latent space dimension (set to what you used in your VAE)
sample_rate = 22050  # Audio sample rate (adjust to match your data)
n_samples = 5  # Number of crow calls to generate
output_dir = "generated_crow_calls"  # Directory to save WAV files

# Create output directory if it doesn't exist
import os
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Load the trained VAE model
vae = load_model('vae_model.keras')
print("‚úÖ Model loaded")

# Access the decoder (assuming VAE has an encoder and decoder)
# If your VAE model doesn't expose the decoder directly, you may need to extract it
# For simplicity, assuming the VAE has a decoder attribute or you can call it directly
decoder = vae.decoder if hasattr(vae, 'decoder') else vae  # Adjust based on your model

# Generate random samples from the latent space
z_samples = np.random.normal(0, 1, (n_samples, latent_dim))

# Generate new crow call data (e.g., spectrograms)
generated_data = decoder.predict(z_samples)
print(f"‚úÖ Generated {n_samples} samples")

# If your model outputs spectrograms, convert them back to audio
for i, data in enumerate(generated_data):
    # Assuming data is a spectrogram (e.g., mel-spectrogram)
    # Convert back to audio using inverse mel-spectrogram (adjust if using another format)
    if data.ndim > 2:  # Remove batch/channel dimensions if present
        data = data.squeeze()

    # Example: Inverse mel-spectrogram to audio
    # Adjust these parameters to match your preprocessing
    mel_spec = data  # Your generated spectrogram
    # If your model outputs normalized spectrograms, denormalize if needed
    # mel_spec = (mel_spec * std) + mean  # If you normalized during preprocessing

    # Convert mel-spectrogram to audio
    audio = librosa.feature.inverse.mel_to_audio(
        mel_spec,
        sr=sample_rate,
        n_fft=2048,
        hop_length=512,
        win_length=2048
    )

    # Save the generated audio as a WAV file
    output_path = os.path.join(output_dir, f"crow_call_{i+1}.wav")
    sf.write(output_path, audio, sample_rate)
    print(f"‚úÖ Saved crow call {i+1} to {output_path}")

print(f"‚úÖ Generated {n_samples} crow calls in {output_dir}")

from google.colab import drive
drive.mount('/content/drive')

# Search across the entire file system for both the SavedModel folder and the .h5 file
!find / -maxdepth 4 \( -type d -name "vae_model.keras" -o -type f -name "vae_full_model.h5" \) 2>/dev/null

import os

# Search from the filesystem root
for root, dirs, files in os.walk("/"):
    if "vae_model.keras" in dirs:
        print("Found SavedModel folder at:", os.path.join(root, "vae_model.keras"))
    if "vae_full_model.h5" in files:
        print("Found H5 model file at:", os.path.join(root, "vae_full_model.h5"))

!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

model_path = 'vae_full_model.h5'

import numpy as np
import librosa
from IPython.display import Audio

# 1) Pick a few indices from your test set
test_idxs = [0, 5, 10]  # for example

# 2) Loop and reconstruct
for idx in test_idxs:
    x = X_test[idx:idx+1]             # shape: (1, H, W, C)
    # encoder.predict returns [z_mean, z_logvar, z_sample]
    z_mean, _, _ = vae.encoder.predict(x)
    spec_rec = vae.decoder.predict(z_mean)  # shape: (1, n_mels, T, 1) or (1, n_mels, T)

    # squeeze channel if needed
    spec_rec = np.squeeze(spec_rec, 0)
    if spec_rec.ndim == 3 and spec_rec.shape[-1] == 1:
        spec_rec = np.squeeze(spec_rec, -1)

    # invert log-mel ‚Üí audio
    mel = np.exp(spec_rec)  # only if you trained on log-mels
    y_rec = librosa.feature.inverse.mel_to_audio(
        mel,
        sr=22050,
        n_fft=1024,
        hop_length=256,
        n_iter=60,
        power=1.0
    )
    print(f"‚ñ∂Ô∏è Reconstruction of test sample #{idx}")
    display(Audio(y_rec, rate=22050))

!find /content -type f -name "*.wav"

import glob, numpy as np, librosa
from IPython.display import Audio
import soundfile as sf

# 1) Pick a real crow file
wav_files = glob.glob("/content/drive/MyDrive/Eywa/American_Crow/*.wav")
wav_path = wav_files[0]
print("Using:", wav_path)

# 2) Load & compute *keyword-only* log‚ÄëMel spectrogram
sr = 22050
n_fft = 1024
hop_length = 256
n_mels = 128

y, _ = librosa.load(path=wav_path, sr=sr)

mel_spec = librosa.feature.melspectrogram(
    y=y,
    sr=sr,
    n_fft=n_fft,
    hop_length=hop_length,
    n_mels=n_mels,
    power=1.0
)
log_mel = np.log(mel_spec + 1e-6)

# 3) Prepare for VAE
x_input = log_mel[np.newaxis, ..., np.newaxis]

# 4) Encode & decode
z_mean, z_logvar, _ = vae.encoder.predict(x_input)
spec_rec = vae.decoder.predict(z_mean)
spec_rec = np.squeeze(spec_rec, 0)
if spec_rec.ndim == 3 and spec_rec.shape[-1] == 1:
    spec_rec = spec_rec[...,0]

# 5) Invert correctly
mel_rec = np.exp(spec_rec)
y_rec = librosa.feature.inverse.mel_to_audio(
    M=mel_rec,
    sr=sr,
    n_fft=n_fft,
    hop_length=hop_length,
    power=1.0,
    n_iter=60
)

# 6) Listen & save
print("‚ñ∂Ô∏è Original")
display(Audio(y, rate=sr))
print("‚ñ∂Ô∏è Reconstruction")
display(Audio(y_rec, rate=sr))
sf.write("reconstructed_call.wav", y_rec, sr)
print("‚úÖ Saved as reconstructed_call.wav")

# 1) Ensure you have tensorflow, librosa, soundfile installed in your env:
#    pip install tensorflow librosa soundfile

import tensorflow as tf
import numpy as np
import glob
import librosa
from IPython.display import Audio
import soundfile as sf
import os
from tensorflow.keras import layers

# 2) Define Sampling and VAE (must match your original)
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        return self.decoder(z)

# 3) Load your saved model (local path)
model_path = 'vae_full_model.h5'  # or wherever you saved it
print("Model exists:", os.path.exists(model_path))
vae = tf.keras.models.load_model(
    model_path,
    custom_objects={"VAE": VAE, "Sampling": Sampling}
)
vae.encoder.summary()

# 4) Reconstruct a real crow call
sr = 22050; n_fft = 1024; hop_length = 256; n_mels = 128
wav_files = glob.glob("American_Crow/*.wav")  # update to your local folder
wav_path = wav_files[0]
print("Reconstructing:", wav_path)

y, _ = librosa.load(path=wav_path, sr=sr)
mel_spec = librosa.feature.melspectrogram(
    y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=1.0
)
log_mel = np.log(mel_spec + 1e-6)
x_input = log_mel[np.newaxis, ..., np.newaxis]

z_mean, z_logvar, _ = vae.encoder.predict(x_input)
spec_rec = vae.decoder.predict(z_mean)
spec_rec = np.squeeze(spec_rec, 0)
if spec_rec.ndim == 3 and spec_rec.shape[-1] == 1:
    spec_rec = spec_rec[...,0]

mel_rec = np.exp(spec_rec)
y_rec = librosa.feature.inverse.mel_to_audio(
    M=mel_rec, sr=sr, n_fft=n_fft, hop_length=hop_length, power=1.0, n_iter=60
)

print("‚ñ∂Ô∏è Original")
display(Audio(y, rate=sr))
print("‚ñ∂Ô∏è Reconstruction")
display(Audio(y_rec, rate=sr))
sf.write("reconstructed_call.wav", y_rec, sr)
print("‚úÖ Saved 'reconstructed_call.wav'")

# 0) If you‚Äôre in Colab, install & mount.
#    If you‚Äôre local, skip the pip/install & Drive mount, and just adjust paths.
!pip install --quiet tensorflow librosa soundfile

from google.colab import drive
drive.mount('/content/drive')

# 1) Define your Sampling layer and VAE class (must match your original)
import tensorflow as tf
from tensorflow.keras import layers

class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * eps

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        return self.decoder(z)

# 2) Load your saved model
model_path = '/content/drive/MyDrive/ColabModels/vae_full_model.h5'
vae = tf.keras.models.load_model(
    model_path,
    custom_objects={"VAE": VAE, "Sampling": Sampling}
)
vae.encoder.summary()

# 3) Pick & reconstruct one real call
import glob, numpy as np, librosa
from IPython.display import Audio
import soundfile as sf

# parameters must match training
sr, n_fft, hop_length, n_mels = 22050, 1024, 256, 128

wav = glob.glob("/content/drive/MyDrive/Eywa/American_Crow/*.wav")[0]
y, _ = librosa.load(path=wav, sr=sr)
mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,
                                     hop_length=hop_length,
                                     n_mels=n_mels, power=1.0)
log_mel = np.log(mel + 1e-6)[None, ..., None]

z_mean, z_logvar, _ = vae.encoder.predict(log_mel)
spec_rec = vae.decoder.predict(z_mean)[0,...,0]
y_rec = librosa.feature.inverse.mel_to_audio(
    M=np.exp(spec_rec), sr=sr,
    n_fft=n_fft, hop_length=hop_length, n_iter=60
)

print("‚ñ∂Ô∏è Original real call"); display(Audio(y, rate=sr))
print("‚ñ∂Ô∏è VAE reconstruction"); display(Audio(y_rec, rate=sr))

# 4) Now sample around that call‚Äôs latent mean
latent_dim = z_mean.shape[-1]
n_gen = 3
sigma = np.exp(0.5 * z_logvar[0])
z_rand = z_mean + 0.3 * sigma * np.random.randn(n_gen, latent_dim)

# 5) Decode & invert those samples
specs = vae.decoder.predict(z_rand)
for i, spec in enumerate(specs):
    if spec.ndim==3: spec = spec[...,0]
    mel_lin = np.exp(spec)
    y_gen = librosa.feature.inverse.mel_to_audio(
        M=mel_lin, sr=sr, n_fft=n_fft, hop_length=hop_length, n_iter=60
    )
    print(f"‚ñ∂Ô∏è Variation #{i+1}"); display(Audio(y_gen, rate=sr))
    sf.write(f"variation_{i+1}.wav", y_gen, sr)

import numpy as np
import librosa
from IPython.display import Audio
import soundfile as sf
import tensorflow as tf

# ‚îÄ‚îÄ‚îÄ 1) Re‚Äëdefine your custom layers/models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# (Make sure these match exactly your original code!)
from tensorflow.keras import layers, Model, Input

class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * eps

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        return self.decoder(z)

# ‚îÄ‚îÄ‚îÄ 2) Re‚Äëbuild your encoder & decoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Replace these with the exact architecture you used in training.

# Example placeholders:
latent_dim = 16   # ‚Üê your latent dimension
n_mels    = 128
timesteps = 64
input_shape = (n_mels, timesteps, 1)

# Encoder
enc_in = Input(shape=input_shape)
x = layers.Conv2D(32, 3, activation="relu", padding="same")(enc_in)
x = layers.Flatten()(x)
z_mean    = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z         = Sampling()([z_mean, z_log_var])
encoder   = Model(enc_in, [z_mean, z_log_var, z], name="encoder")

# Decoder
dec_in = Input(shape=(latent_dim,))
x = layers.Dense(n_mels * timesteps, activation="relu")(dec_in)
x = layers.Reshape((n_mels, timesteps, 1))(x)
dec_out = layers.Conv2DTranspose(1, 3, activation="sigmoid", padding="same")(x)
decoder = Model(dec_in, dec_out, name="decoder")

# ‚îÄ‚îÄ‚îÄ 3) Instantiate VAE and load weights ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
vae = VAE(encoder, decoder)
# Note: no compile needed for inference
model_path = "/content/drive/MyDrive/ColabModels/vae_full_model.h5"
print("Loading weights from:", model_path)
vae.load_weights(model_path)
print("‚úÖ Weights loaded.")

# ‚îÄ‚îÄ‚îÄ 4) Sample new crow calls ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
n_samples = 3
sr        = 22050
n_fft     = 1024
hop_length= 256
n_iter    = 60

# Sample ‚Äúin‚Äëdistribution‚Äù: small Gaussian around zero
z_rand = np.random.normal(loc=0.0, scale=0.3, size=(n_samples, latent_dim))

# Decode to log‚ÄëMel spec, then invert
specs = vae.decoder.predict(z_rand)
# drop channel dim if present
if specs.ndim == 4 and specs.shape[-1] == 1:
    specs = specs[...,0]

for i, spec in enumerate(specs, start=1):
    # if your decoder outputs log‚Äëmels
    mel = np.exp(spec)
    y   = librosa.feature.inverse.mel_to_audio(
        M=mel, sr=sr, n_fft=n_fft, hop_length=hop_length, n_iter=n_iter
    )
    print(f"‚ñ∂Ô∏è Generated call #{i}")
    display(Audio(y, rate=sr))
    sf.write(f"generated_call_{i}.wav", y, sr)

print("‚úÖ Done ‚Äî check your working directory for generated_call_1.wav, etc.")

# 0) Mount Google Drive so your saved model is accessible
from google.colab import drive
drive.mount('/content/drive')

# 1) (Re)load your VAE if needed ‚Äì skip this if 'vae' is already in memory
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras import layers

# Define your Sampling layer and VAE class exactly as before
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * eps

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        return self.decoder(z)

# Point to your saved H5 (or adjust if you use load_weights)
model_path = '/content/drive/MyDrive/ColabModels/vae_full_model.h5'
vae = load_model(
    model_path,
    custom_objects={"VAE": VAE, "Sampling": Sampling}
)

# 2) Generate 3 ‚Äúin‚Äëdistribution‚Äù samples (¬±30% œÉ around zero)
import numpy as np, librosa
from IPython.display import Audio
import soundfile as sf

n_samples  = 3
latent_dim = vae.encoder.output_shape[0][1]
z_rand     = np.random.normal(loc=0.0, scale=0.3, size=(n_samples, latent_dim))

# Decode to log‚ÄëMel spec
specs = vae.decoder.predict(z_rand)
if specs.ndim == 4 and specs.shape[-1] == 1:
    specs = specs[...,0]

# Invert & play/save
sr         = 22050
n_fft      = 1024
hop_length = 256
n_iter     = 60

for i, spec in enumerate(specs, start=1):
    # undo log if needed
    if spec.min() < 0:
        spec = np.exp(spec)
    y_gen = librosa.feature.inverse.mel_to_audio(
        M=spec, sr=sr, n_fft=n_fft, hop_length=hop_length, n_iter=n_iter
    )
    print(f"‚ñ∂Ô∏è Generated call #{i}")
    display(Audio(y_gen, rate=sr))
    sf.write(f"generated_call_{i}.wav", y_gen, sr)

print("‚úÖ Done‚Äîcheck generated_call_1.wav, generated_call_2.wav, generated_call_3.wav")

import tensorflow as tf
from tensorflow.keras import layers, Model, Input
import numpy as np
import librosa
from IPython.display import Audio
import soundfile as sf
import glob, os

# ‚îÄ‚îÄ‚îÄ 1) Mount Drive (Colab only) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# ‚îÄ‚îÄ‚îÄ 2) Define Sampling & VAE (copy *your* original code!) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * eps

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        return self.decoder(z)

# ‚îÄ‚îÄ‚îÄ 3) Rebuild your encoder & decoder exactly as you trained them ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     **You must replace the lines below with your real architecture.**
latent_dim = 16         # ‚Üê your latent size
n_mels    = 128         # ‚Üê your mel bins
timesteps = 64          # ‚Üê your spectrogram time frames
input_shape = (n_mels, timesteps, 1)

# ‚Üí ENCODER
enc_in = Input(shape=input_shape)
x = layers.Conv2D(32, 3, activation="relu", padding="same")(enc_in)
x = layers.Flatten()(x)
z_mean    = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z         = Sampling()([z_mean, z_log_var])
encoder   = Model(enc_in, [z_mean, z_log_var, z], name="encoder")

# ‚Üí DECODER
dec_in = Input(shape=(latent_dim,))
x = layers.Dense(units=n_mels * timesteps, activation="relu")(dec_in)
x = layers.Reshape((n_mels, timesteps, 1))(x)
dec_out = layers.Conv2DTranspose(1, 3, activation="sigmoid", padding="same")(x)
decoder = Model(dec_in, dec_out, name="decoder")

# ‚îÄ‚îÄ‚îÄ 4) Instantiate your VAE and load weights ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
vae = VAE(encoder, decoder)
weights_path = "/content/drive/MyDrive/ColabModels/vae_full_model.h5"
print("Loading weights from:", weights_path)
vae.load_weights(weights_path)
print("‚úÖ Weights loaded into your VAE instance")

# ‚îÄ‚îÄ‚îÄ 5) Generate 3 in‚Äëdistribution crow calls ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
n_gen = 3
sr     = 22050
n_fft      = 1024
hop_length = 256
n_iter     = 60

# Sample around zero with reduced variance
z_rand = np.random.normal(loc=0.0, scale=0.3, size=(n_gen, latent_dim))

# Decode to (log‚Äë)Mel specs
specs = vae.decoder.predict(z_rand)
if specs.ndim == 4 and specs.shape[-1] == 1:
    specs = specs[...,0]

for i, spec in enumerate(specs, start=1):
    # undo log if you used log‚Äëmels
    if spec.min() < 0:
        spec = np.exp(spec)
    y = librosa.feature.inverse.mel_to_audio(
        M=spec, sr=sr, n_fft=n_fft, hop_length=hop_length, n_iter=n_iter
    )
    print(f"‚ñ∂Ô∏è Generated call #{i}")
    display(Audio(y, rate=sr))
    sf.write(f"generated_call_{i}.wav", y, sr)

print("‚úÖ Done‚Äîlook for generated_call_1.wav, ‚Ä¶ in your working folder")

import tensorflow as tf
from tensorflow.keras import layers, Model, Input

# ‚îÄ‚îÄ‚îÄ 1) Define Sampling layer and VAE subclass ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        # add KL divergence loss
        kl = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
        self.add_loss(kl)
        return self.decoder(z)

# ‚îÄ‚îÄ‚îÄ 2) Build your real encoder & decoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Adjust these parameters to match exactly what you used
n_mels     = 128
timesteps  = 64
latent_dim = 16
input_shape = (n_mels, timesteps, 1)

# ‚Äî Encoder ‚Äî
enc_inputs = Input(shape=input_shape, name="encoder_input")
x = layers.Conv2D(32, 3, strides=2, padding="same", activation="relu")(enc_inputs)
x = layers.Conv2D(64, 3, strides=2, padding="same", activation="relu")(x)
shape_before_flatten = tf.keras.backend.int_shape(x)[1:]  # e.g. (32, 16, 64)
x = layers.Flatten()(x)
x = layers.Dense(128, activation="relu")(x)
z_mean    = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z         = Sampling()([z_mean, z_log_var])
encoder   = Model(enc_inputs, [z_mean, z_log_var, z], name="encoder")

# ‚Äî Decoder ‚Äî
dec_inputs = Input(shape=(latent_dim,), name="z_sampling")
x = layers.Dense(units=tf.math.reduce_prod(shape_before_flatten), activation="relu")(dec_inputs)
x = layers.Reshape(target_shape=shape_before_flatten)(x)
x = layers.Conv2DTranspose(64, 3, strides=2, padding="same", activation="relu")(x)
x = layers.Conv2DTranspose(32, 3, strides=2, padding="same", activation="relu")(x)
dec_outputs = layers.Conv2DTranspose(1, 3, padding="same", activation="sigmoid")(x)
decoder     = Model(dec_inputs, dec_outputs, name="decoder")

# ‚îÄ‚îÄ‚îÄ 3) Instantiate, compile & train the VAE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
vae = VAE(encoder, decoder)
vae.compile(optimizer=tf.keras.optimizers.Adam(),
            loss=tf.keras.losses.MeanSquaredError())

# Now train to reconstruct your spectrograms
# X_train and X_test should be arrays of shape (N, n_mels, timesteps, 1)
vae.fit(
    X_train, X_train,
    validation_data=(X_test, X_test),
    batch_size=64,
    epochs=50,
    verbose=1
)
print("‚úÖ Training complete ‚Äî `vae` is in memory for sampling!")

import tensorflow as tf
from tensorflow.keras import layers, Model, Input
import numpy as np

# ‚îÄ‚îÄ‚îÄ 1) Define Sampling layer and VAE subclass ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        kl_loss = -0.5 * tf.reduce_mean(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
        )
        self.add_loss(kl_loss)
        return self.decoder(z)

# ‚îÄ‚îÄ‚îÄ 2) Build encoder & decoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
n_mels     = 128
timesteps  = 64
latent_dim = 16
input_shape = (n_mels, timesteps, 1)

# Encoder
enc_inputs = Input(shape=input_shape, name="encoder_input")
x = layers.Conv2D(32, 3, strides=2, padding="same", activation="relu")(enc_inputs)
x = layers.Conv2D(64, 3, strides=2, padding="same", activation="relu")(x)
# record shape for decoder
shape_before_flatten = x.shape[1:]           # e.g. (32, 16, 64)
flatten_units       = int(np.prod(shape_before_flatten))
x = layers.Flatten()(x)
x = layers.Dense(128, activation="relu")(x)
z_mean    = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z         = Sampling()([z_mean, z_log_var])
encoder   = Model(enc_inputs, [z_mean, z_log_var, z], name="encoder")

# Decoder
dec_inputs = Input(shape=(latent_dim,), name="z_sampling")
x = layers.Dense(units=flatten_units, activation="relu")(dec_inputs)
x = layers.Reshape(target_shape=shape_before_flatten)(x)
x = layers.Conv2DTranspose(64, 3, strides=2, padding="same", activation="relu")(x)
x = layers.Conv2DTranspose(32, 3, strides=2, padding="same", activation="relu")(x)
dec_outputs = layers.Conv2DTranspose(1, 3, padding="same", activation="sigmoid")(x)
decoder     = Model(dec_inputs, dec_outputs, name="decoder")

# ‚îÄ‚îÄ‚îÄ 3) Instantiate, compile & train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
vae = VAE(encoder, decoder)
vae.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.MeanSquaredError()
)

# X_train, X_test should be arrays of shape (N, n_mels, timesteps, 1)
vae.fit(
    X_train, X_train,
    validation_data=(X_test, X_test),
    batch_size=64,
    epochs=50,
    verbose=1
)
print("‚úÖ Training complete ‚Äî `vae` is alive for sampling!")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 0) If in Colab, mount Drive; otherwise skip
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# 1) Load & preprocess your crow audio into X_train / X_test
import glob, librosa, numpy as np
from sklearn.model_selection import train_test_split

# audio & spectrogram parameters
sr         = 22050
n_fft      = 1024
hop_length = 256
n_mels     = 128
max_len    = 64        # number of time‚Äêframes per spectrogram

# 1A) Find all your .wav files
wav_paths = glob.glob("/content/drive/MyDrive/Eywa/American_Crow/*.wav")
print("Found", len(wav_paths), "files.")

# 1B) Load, compute log‚ÄëMel, pad/truncate to (n_mels √ó max_len)
specs = []
for p in wav_paths:
    y, _ = librosa.load(path=p, sr=sr)
    m = librosa.feature.melspectrogram(
        y=y, sr=sr,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels,
        power=1.0
    )
    log_m = np.log(m + 1e-6)
    # pad or truncate time axis
    if log_m.shape[1] < max_len:
        pad = max_len - log_m.shape[1]
        log_m = np.pad(log_m, ((0,0),(0,pad)), mode="constant")
    else:
        log_m = log_m[:, :max_len]
    specs.append(log_m)

# stack and add channel dim
X = np.stack(specs)[..., np.newaxis]
print("X shape:", X.shape)  # ‚Üí (N, n_mels, max_len, 1)

# split
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)
print("Train/test:", X_train.shape, X_test.shape)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2) Define your Sampling layer and VAE subclass
import tensorflow as tf
from tensorflow.keras import layers, Model, Input

class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * eps

class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        kl = -0.5 * tf.reduce_mean(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
        )
        self.add_loss(kl)
        return self.decoder(z)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3) Build your real encoder & decoder architectures

latent_dim = 16   # <-- use the same latent size you trained with
input_shape = (n_mels, max_len, 1)

# Encoder
enc_in = Input(shape=input_shape, name="encoder_input")
x = layers.Conv2D(32, 3, strides=2, padding="same", activation="relu")(enc_in)
x = layers.Conv2D(64, 3, strides=2, padding="same", activation="relu")(x)
shape_before = x.shape[1:]               # e.g. (32, 16, 64)
flat_units   = int(np.prod(shape_before))
x = layers.Flatten()(x)
x = layers.Dense(128, activation="relu")(x)
z_mean    = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z         = Sampling()([z_mean, z_log_var])
encoder   = Model(enc_in, [z_mean, z_log_var, z], name="encoder")

# Decoder
dec_in = Input(shape=(latent_dim,), name="decoder_input")
y = layers.Dense(flat_units, activation="relu")(dec_in)
y = layers.Reshape(target_shape=shape_before)(y)
y = layers.Conv2DTranspose(64, 3, strides=2, padding="same", activation="relu")(y)
y = layers.Conv2DTranspose(32, 3, strides=2, padding="same", activation="relu")(y)
dec_out = layers.Conv2DTranspose(1, 3, padding="same", activation="sigmoid")(y)
decoder = Model(dec_in, dec_out, name="decoder")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4) Instantiate, compile & train the VAE
vae = VAE(encoder, decoder)
vae.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.MeanSquaredError()
)

# this will leave `vae` alive in memory
history = vae.fit(
    X_train, X_train,
    validation_data=(X_test, X_test),
    batch_size=64,
    epochs=50,
    verbose=1
)

print("‚úÖ Training complete ‚Äî `vae` is in memory for sampling!")

vae         # your trained VAE
X_train     # your training spectrograms
X_test      # your test spectrograms

import numpy as np
import librosa
from IPython.display import Audio
import soundfile as sf

# ‚îÄ‚îÄ‚îÄ Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
sr         = 22050    # your training sampling rate
n_fft      = 1024
hop_length = 256
n_iter     = 60
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Suppose `specs` is your array of shape (n_samples, n_mels, timesteps) or (n_samples, n_mels, timesteps, 1)
# If there's a singleton channel dim, squeeze it:
if specs.ndim == 4 and specs.shape[-1] == 1:
    specs = specs[..., 0]

n_samples = specs.shape[0]

for i in range(n_samples):
    log_mel = specs[i]

    # 1) If these are log‚Äëmels, invert the log back to linear
    if log_mel.min() < 0:
        mel_lin = np.exp(log_mel)
    else:
        mel_lin = log_mel

    # 2) Invert mel‚Äëspectrogram to waveform
    y = librosa.feature.inverse.mel_to_audio(
        M=mel_lin,
        sr=sr,
        n_fft=n_fft,
        hop_length=hop_length,
        power=1.0,
        n_iter=n_iter
    )

    # 3) Play in‚Äënotebook and save as WAV
    print(f"‚ñ∂Ô∏è Sample #{i+1}")
    display(Audio(y, rate=sr))
    sf.write(f"generated_call_{i+1}.wav", y, sr)

print("‚úÖ All done‚Äîcheck generated_call_*.wav in your working directory.")```

This will:
1. **Undo** the natural log (if your specs are log‚ÄëMel).
2. **Run Griffin‚ÄëLim** with 60 iterations (`n_iter`) to reconstruct a time‚Äëdomain signal.
3. **Play** each sample inline and **save** it as `generated_call_1.wav`, `generated_call_2.wav`, etc.

import numpy as np
import librosa
from IPython.display import Audio
import soundfile as sf

# Parameters
sr = 22050
n_fft = 1024
hop_length = 256
n_iter = 60

# Assume `specs` is your numpy array of shape (n_samples, n_mels, timesteps)
# or (n_samples, n_mels, timesteps, 1)
if specs.ndim == 4 and specs.shape[-1] == 1:
    specs = specs[..., 0]

n_samples = specs.shape[0]

for i in range(n_samples):
    log_mel = specs[i]
    # Invert the log if needed
    mel = np.exp(log_mel) if log_mel.min() < 0 else log_mel

    # Reconstruct waveform via Griffin-Lim
    y = librosa.feature.inverse.mel_to_audio(
        M=mel,
        sr=sr,
        n_fft=n_fft,
        hop_length=hop_length,
        power=1.0,
        n_iter=n_iter
    )

    # Play and save
    print(f"‚ñ∂Ô∏è Sample {i+1}")
    display(Audio(y, rate=sr))
    sf.write(f"generated_call_{i+1}.wav", y, sr)

import numpy as np
import librosa
from IPython.display import Audio
import soundfile as sf

# Parameters
sr = 22050
n_fft = 1024
hop_length = 256
n_iter = 60

# 0) If specs is a Python list, convert to a NumPy array
if isinstance(specs, list):
    specs = np.stack(specs)

# 1) If there‚Äôs a channel axis of size 1, squeeze it away
if specs.ndim == 4 and specs.shape[-1] == 1:
    specs = specs[..., 0]

n_samples = specs.shape[0]

# 2) Loop over each spectrogram
for i in range(n_samples):
    log_mel = specs[i]

    # 3) Undo log if your specs are log‚Äëmels
    mel = np.exp(log_mel) if log_mel.min() < 0 else log_mel

    # 4) Invert mel‚Äëspectrogram back to waveform
    y = librosa.feature.inverse.mel_to_audio(
        M=mel,
        sr=sr,
        n_fft=n_fft,
        hop_length=hop_length,
        power=1.0,
        n_iter=n_iter
    )

    # 5) Play inline and save to disk
    print(f"‚ñ∂Ô∏è Sample {i+1}")
    display(Audio(y, rate=sr))
    sf.write(f"generated_call_{i+1}.wav", y, sr)

from IPython.display import FileLink
FileLink('generated_crow_calls.zip')

import os
import zipfile

# 1) Where your generated .wav files live
workdir = '/content'
zip_path = os.path.join(workdir, 'generated_crow_calls.zip')

# 2) Create the ZIP
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:
    for fname in os.listdir(workdir):
        if fname.startswith('generated_call_') and fname.endswith('.wav'):
            z.write(os.path.join(workdir, fname), arcname=fname)
print(f"üì¶ Created archive at {zip_path}")

# 3) Download via Colab
from google.colab import files
files.download(zip_path)

